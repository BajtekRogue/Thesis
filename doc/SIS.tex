\section{Pewne wygaśnięcie}

Analizę modelu \textbf{SIS} zacznijmy od formalnego udowodnienia pewności wygaśnięcia infekcji.

\begin{theorem}\label{theorem:infection_dies_out_SIS}
Niech $G=(V,E)$ będzie grafem spójnym, $s\in V$ źródłem infekcji oraz $v\in V\setminus\{s\}$.
Wtedy zachodzą następujące tożsamości:
\[
    \mathbb{P}[X_v = \infty] > 0, \quad \lim_{t\to\infty} \mathbb{E}[Y_t] = 0, \quad \mathbb{P}[Z < \infty] = 1.
\]
\end{theorem}

\begin{proof}
Dowód $\mathbb{P}[X_v = \infty] > 0$ oraz $\mathbb{P}[Z < \infty] = 1$ wynika z \Cref{theorem:infection_dies_out_SIR}.
Skoro infekcja prawie na pewno wymrze to $\mathbf{1}_{Y_t} \xrightarrow{\text{a.s}} 0$ a więc i $Y_t \xrightarrow{\text{a.s}} 0$.
Zauważmy, że $0\le Y_t \le |V|$ dla każdego $t$.
Zatem z Twierdzenia Lebesgue'a o zbieżności ograniczonej otrzymujemy
\[
    \lim_{t\to\infty} \mathbb{E}[Y_t] = \mathbb{E}[\lim_{t\to\infty} Y_t] = \mathbb{E}[0]=0.
\]
\end{proof}


\section{Dwa wierzchołki, jedna krawędź}

Standardowo na pierwszy przykład grafu rozważamy graf o wierzchołkach $V=\{u,v\}$ oraz krawędziach $E=\{\{u,v\}\}$ oraz wierzchołku początkowym $u$.
Istnieją cztery możliwe stany systemu: $\mathcal{I}_t=\varnothing, \; \mathcal{I}_t=\{u\}, \; \mathcal{I}_t=\{v\}, \; \mathcal{I}_t=\{u,v\}$. 
Stan, w którym żaden wierzchołek nie jest zainfekowany, jest stanem absorbującym. 
Ponadto, z każdego pozostałego stanu możemy przejść do dowolnego innego. 
Rozkład $X_v$ będzie tutaj taki sam jak dla modelu \textbf{SIR} a więc
\[
    X_v\sim \mathrm{KGeo}(p,\alpha),
\]
oraz
\[
    \mathbb{E}[X_v|X_v<\infty]=\frac{1}{1-q\beta}.
\]

Następnie spróbujmy wyznaczyć rozkład $Y_t$. 
Zauważmy, że $Y_t\in\{0,1,2\}$. 
Prawdopodobieństwa przejść są następujące:
\begin{equation*}
\begin{aligned}
\mathbb{P}[Y_{t+1} = 0 | Y_t = 0 ] &= 1,\\
\mathbb{P}[Y_{t+1} = 0 | Y_t = 1 ] &= q\alpha,\\
\mathbb{P}[Y_{t+1} = 0 | Y_t = 2 ] &= \alpha^2,\\
\mathbb{P}[Y_{t+1} = 1 | Y_t = 0 ] &= 0,\\
\mathbb{P}[Y_{t+1} = 1 | Y_t = 1 ] &= p\alpha+q\beta,\\
\mathbb{P}[Y_{t+1} = 1 | Y_t = 2 ] &= 2\alpha\beta,\\
\mathbb{P}[Y_{t+1} = 2 | Y_t = 0 ] &= 0,\\
\mathbb{P}[Y_{t+1} = 2 | Y_t = 1 ] &= p\beta,\\
\mathbb{P}[Y_{t+1} = 2 | Y_t = 2 ] &= \beta^2.
\end{aligned}
\end{equation*}
Z wzoru na prawdopodobieństwo całkowite dla $k\in \{0,1,2\}$ mamy:
\[
    \mathbb{P}[Y_{t+1} = k] = \sum_{j=0}^{2} \mathbb{P}[Y_{t+1}=k | Y_t=j]\cdot \mathbb{P}[Y_t=j].
\]
Oznaczmy $a_t=\mathbb{P}[Y_t=0], \; b_t=\mathbb{P}[Y_t=1], \; c_t=\mathbb{P}[Y_t=2]$. 
Oczywiście $a_0=0, \; b_0=1, \; c_0=0$. 
Stąd otrzymujemy układ równań rekurencyjnych:
\[
\begin{cases}
    a_{t+1} = a_t + q\alpha\cdot b_t + \alpha^2 \cdot c_t\\
    b_{t+1} = (p\alpha+q\beta)\cdot b_t + 2\alpha\beta\cdot c_t\\
    c_{t+1} = p\beta\cdot b_t + \beta^2\cdot c_t.
\end{cases}
\]
Połóżmy
\[
\mathbf{P} = 
\begin{bmatrix}
1 & q\alpha & \alpha^2 \\
0 & p\alpha+q\beta  & 2\alpha\beta \\
0 & p\beta & \beta^2
\end{bmatrix}, \quad \mathbf{y}_t = \begin{bmatrix}
a_t \\
b_t \\
c_t
\end{bmatrix}.
\]
Wtedy $\mathbf{y}_{t+1}=\mathbf{P}\mathbf{y}_t$, a więc $\mathbf{y}_{t}=\mathbf{P}^t\mathbf{y}_0$. 
Jeśli chodzi o wartość oczekiwaną, to mamy $\mathbb{E}[Y_t]=0\cdot a_t + 1\cdot b_t + 2\cdot c_t = b_t+2c_t$. 
Z \Cref{theorem:infection_dies_out_SIS} mamy
\[
    \lim_{t\to\infty} \mathbb{E}[Y_t] = 0.
\]

Przyjrzyjmy się teraz zmiennej $Z$. 
Mamy $\mathbb{P}[Z>t]=\mathbb{P}[Y_t\ne 0] = b_t + c_t$. Oznaczmy
\[
\mathbf{Q} = 
\begin{bmatrix}
p\alpha+q\beta  & 2\alpha\beta \\p\beta & \beta^2
\end{bmatrix}, 
\quad \mathbf{z}_t = \begin{bmatrix}
b_t \\
c_t
\end{bmatrix},
\quad \mathbf{1} = \begin{bmatrix}
1 \\
1
\end{bmatrix}.
\]
Wtedy $\mathbb{P}[Z>t] = \mathbf{1}^\top \mathbf{z}_t = \mathbf{1}^\top \mathbf{Q}^t \mathbf{z}_0$. 
Mamy więc
\[
    \mathbb{E}[Z] = \sum_{t=0}^{\infty} \mathbb{P}[Z>t] = \sum_{t=0}^{\infty} \mathbf{1}^\top\mathbf{Q}^t \mathbf{z}_0 = \mathbf{1}^\top{(\mathbf{I}-\mathbf{Q})}^{-1} \mathbf{z}_0.
\]
Dalej:
\[
    {(\mathbf{I} - \mathbf{Q})}^{-1} = 
    \begin{bmatrix}
    1 - p\alpha-q\beta & -2\alpha\beta \\ -p\beta & 1 -\beta^2
    \end{bmatrix}^{-1}
    =   \frac{1}{\Delta}  
    \begin{bmatrix}
    1 -\beta^2 & 2\alpha\beta \\ p\beta & 1 - p\alpha-q\beta 
    \end{bmatrix},
\]
oraz
\[
    \mathbf{1}^\top{(\mathbf{I}-\mathbf{Q})}^{-1} \mathbf{z}_0 = \mathbf{1}^\top \frac{1}{\Delta} \begin{bmatrix}
        1 -\beta^2 \\
        p\beta
    \end{bmatrix}
    = \frac{1-\beta^2+p\beta}{\Delta},
\]
gdzie $\Delta=\det(\mathbf{I} - \mathbf{Q})$. 
Obliczmy teraz ten wyznacznik:
\begin{equation*}
\begin{aligned}
\Delta &=(1 - p\alpha-q\beta)(1 -\beta^2) - (-2\alpha\beta)(-p\beta) \\
&= 1-p\alpha-q\beta-\beta^2+p\alpha\beta^2+q\beta^3-2p\alpha\beta \\
&=(1-\beta^2)(1-q\beta)-p\alpha(1+\beta^2)\\
&= (1-{(1-\alpha)}^2)(1-(1-p)(1-\alpha))-p\alpha(1+{(1-\alpha)}^2) \\
&=\alpha(2-\alpha)(p+\alpha-p\alpha)-p\alpha(2-2\alpha+\alpha^2) \\
&=\alpha(2p+2\alpha-2p\alpha-\alpha p-\alpha^2+p\alpha^2-2p+2\alpha p-p\alpha^2) \\
&=\alpha^2(2-p-\alpha)=\alpha^2(q+\beta).
\end{aligned}
\end{equation*}
Ostatecznie otrzymujemy:
\[
    \mathbb{E}[Z] = \frac{1-\beta^2+p\beta}{\alpha^2(q+\beta)}.
\]


\section{Grafy pełne}

Po analizie dla $\mathrm{K}_2$ postarajmy się ją uogólnić dla $\mathrm{K}_n$.
Ze względu na symetrie dowolny stan z taką samą liczbą zainfekowanych wierzchołków jest izomorficzny.
Z \Cref{theorem:infection_dies_out_SIS} wiemy, że $\mathbb{E}[Y_t]\to 0$.
Ustalmy $t\in\mathbb{N}$.
Niech $A_t$ będzie zmienną losową oznaczająca liczbę wierzchołków, które przetrwają rundę $t$ oraz $B_t$ liczbę nowo zainfekowanych wierzchołków w tej rundzie.
Formalnie 
\[
    A_t=|\{v\in V: v \in \mathcal{S}_t \cap \mathcal{S}_{t+1}\}|, \quad B_t=|\{v\in V: v\in\mathcal{S}_t\cap \mathcal{I}_{t+1}\}|.
\]
Oznaczmy $i=Y_t$ oraz $j=Y_{t+1}$. 
Każdy zarażony wierzchołek może przetrwać niezależnie od siebie w formacie próby Bernoulliego.
A więc $A_t\sim \mathrm{Bin}(i,\beta)$.
Dalej każdy podatny wierzchołek może zostać zarażony niezależnie przez $n-i$ sąsiadów.
Szansa, że któremuś z nich się uda wynosi $1-q^i$.
Stąd $B_t\sim\mathrm{Bin}(n-i,1-q^i)$.
Ponadto $Y_{t+1}=A_t+B_t$.
Możemy policzyć warunkową liczbę oczekiwanych zarażeń w $t+1$ kroku:
\[
    \mathbb{E}[Y_{t+1}|Y_t=i]=\mathbb{E}[A_t+B_t]=i\beta+(n-i)(1-q^i).
\]
Oznaczmy $P_{i\to j}=\mathbb{P}[Y_{t+1}=j|Y_t=i]$ dla $j\in\{0,1,\dots,n\}$.
Wtedy korzystając z \Cref{fact:sum_of_bin_RV_2} mamy
\[
    P_{i\to j} = \sum_{\ell=\max\{0,j-(n-i)\}}^{\min\{i,j\}} \binom{i}{\ell}\binom{n-i}{j-\ell}\beta^\ell\alpha^{i-\ell}(1-q^i)^{j-\ell}(q^i)^{n-i-(j-\ell)}.
\]
Jest to uogólnienie prawdopodobieństw przejść, które wyznaczyliśmy wcześniej dla $n=2$.
Ponownie z prawdopodobieństwa całkowitego dla $j\in\{0,1,\dots,n\}$ mamy
\[
    \mathbb{P}[Y_{t+1}=j]=\sum_{i=0}^{n}\mathbb{P}[Y_{t+1}=j|Y_t=i]\cdot\mathbb{P}[Y_t=i].
\]
Zdefiniujmy wektor
\[
    \mathbf{y}_t= \begin{bmatrix}
        \mathbb{P}[Y_t=0] \\
        \mathbb{P}[Y_t=1] \\
         \vdots \\ 
        \mathbb{P}[Y_t=n] \\
    \end{bmatrix}
\]
Wtedy $\mathbf{y}_t^{(k)}=\mathbb{P}[Y_t=k]$ dla $k\in\{0,1,\dots,n\}$.
W uproszczonej notacji możemy zapisać
\[
    \mathbf{y}_{t+1}^{(j)}=\sum_{i=0}^{n} P_{i\to j} \cdot \mathbf{y}_t^{(i)}.
\]
Macierz przejść dla naszego łańcucha Markova dana jest
\[
    \mathbf{P}=\begin{bmatrix} P_{i\to j}\end{bmatrix}_{0\le i,j\le n}.
\]
Mamy $\mathbf{y}_{t+1}=\mathbf{P}\mathbf{y}_t$ jak i $\mathbf{y}_t=\mathbf{P}^t\mathbf{y}_0$.
Zbudowanie tej macierzy jest możliwe w czasie $\mathcal{O}(n^3)$.

Jeśli chodzi o zmienną $Z$ to mamy $\mathbb{P}[Z>t]=\mathbb{P}[Y_t\ne 0]$.
Kładziemy
\[
    \mathbf{Q}=\begin{bmatrix}P_{i\to j}\end{bmatrix}_{1\le i,j\le n}, \quad \mathbf{z}_t=\begin{bmatrix}\mathbb{P}[Y_t]=k\end{bmatrix}_{1\le k \le n}, \quad \mathbf{1} = \begin{bmatrix} 1 \end{bmatrix}_{1\le k \le n}.
\]
Licząc wartość oczekiwaną dostajemy
\[
    \mathbb{E}[Z] = \sum_{t=0}^{\infty} \mathbb{P}[Z>t] = \sum_{t=0}^{\infty} \mathbf{1}^\top\mathbf{z}_t= \sum_{t=0}^{\infty} \mathbf{1}^\top\mathbf{Q}^t \mathbf{z}_0 = \mathbf{1}^\top{(\mathbf{I}-\mathbf{Q})}^{-1} \mathbf{z}_0.
\]